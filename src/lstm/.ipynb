{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "df_folda = '/Users/sango.m.ab/Desktop/research/script/anaphora/src/lstm/dataframe'\n",
    "domain_dict = {'OC':'Yahoo!知恵袋','OW':'白書','OY':'Yahoo!ブログ',\n",
    "    'PB':'書籍','PM':'雑誌','PN':'新聞'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(df_folda):\n",
    "    dataset_dict = {}\n",
    "    for domain in domain_dict:\n",
    "        with open(f'{df_folda}/dataframe_list_{domain}.pickle', 'rb') as f:\n",
    "            df_list = pickle.load(f)\n",
    "        dataset = []\n",
    "        for df in df_list:\n",
    "            y_ga = np.array(df['ga_case'], dtype=np.int32)\n",
    "            y_o = np.array(df['o_case'], dtype=np.int32)\n",
    "            y_ni = np.array(df['ni_case'], dtype=np.int32)\n",
    "            df = df.drop('ga_case', axis=1).drop('o_case', axis=1).drop('ni_case', axis=1).drop('ga_dep_tag', axis=1).drop('o_dep_tag', axis=1).drop('ni_dep_tag', axis=1)\n",
    "            x = np.array(df, dtype=np.float32)\n",
    "#             y_ga_index = np.array([y_ga.argmax()], dtype=np.int32)\n",
    "            dataset.append((x, y_ga))\n",
    "        dataset_dict[domain] = dataset\n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataframe/dataframe_list_OC.pickle', 'rb') as f:\n",
    "    df_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for df in df_list:\n",
    "    y_ga = np.array(df['ga_case'], dtype=np.int32)\n",
    "    y_o = np.array(df['o_case'], dtype=np.int32)\n",
    "    y_ni = np.array(df['ni_case'], dtype=np.int32)\n",
    "    df = df.drop('ga_case', axis=1).drop('o_case', axis=1).drop('ni_case', axis=1).drop('ga_dep_tag', axis=1).drop('o_dep_tag', axis=1).drop('ni_dep_tag', axis=1)\n",
    "    x = np.array(df, dtype=np.float32)\n",
    "#     y_ga_index = np.array([y_ga.argmax()], dtype=np.int32)\n",
    "    dataset.append((x, y_ga))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import training\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 91\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = chainer.iterators.SerialIterator(dataset[:11000], batchsize)\n",
    "test_iter = chainer.iterators.SerialIterator(dataset[11000:], batchsize,\n",
    "                                             repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(chainer.Chain):\n",
    "\n",
    "    \"\"\"A LSTM-RNN Encoder with Word Embedding.\n",
    "\n",
    "    This model encodes a sentence sequentially using LSTM.\n",
    "\n",
    "    Args:\n",
    "        n_layers (int): The number of LSTM layers.\n",
    "        n_vocab (int): The size of vocabulary.\n",
    "        n_units (int): The number of units of a LSTM layer and word embedding.\n",
    "        dropout (float): The dropout ratio.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_units, dropout=0.1):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.encoder = L.NStepLSTM(n_layers, n_units, n_units, dropout)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.out_units = n_units\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        last_h, last_c, ys = self.encoder(None, None, xs)\n",
    "        assert(last_h.shape == (self.n_layers, len(xs), self.out_units))\n",
    "        concat_outputs = last_h[-1]\n",
    "        return concat_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = RNNEncoder(n_layers=1, n_units=234, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(chainer.Chain):\n",
    "\n",
    "    \"\"\"A classifier using a given encoder.\n",
    "\n",
    "     This chain encodes a sentence and classifies it into classes.\n",
    "\n",
    "     Args:\n",
    "         encoder (Link): A callable encoder, which extracts a feature.\n",
    "             Input is a list of variables whose shapes are\n",
    "             \"(sentence_length, )\".\n",
    "             Output is a variable whose shape is \"(batchsize, n_units)\".\n",
    "         n_class (int): The number of classes to be predicted.\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, n_class, dropout=0.1):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.encoder = encoder\n",
    "            self.output = L.Linear(encoder.out_units, n_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def __call__(self, xs, ys):\n",
    "        concat_outputs = self.predict(xs)\n",
    "        concat_truths = F.concat(ys, axis=0)\n",
    "\n",
    "        loss = F.softmax_cross_entropy(concat_outputs, concat_truths)\n",
    "        accuracy = F.accuracy(concat_outputs, concat_truths)\n",
    "        reporter.report({'loss': loss.data}, self)\n",
    "        reporter.report({'accuracy': accuracy.data}, self)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, xs, softmax=False, argmax=False):\n",
    "        concat_encodings = F.dropout(self.encoder(xs), ratio=self.dropout)\n",
    "        concat_outputs = self.output(concat_encodings)\n",
    "        if softmax:\n",
    "            return F.softmax(concat_outputs).data\n",
    "        elif argmax:\n",
    "            return self.xp.argmax(concat_outputs.data, axis=1)\n",
    "        else:\n",
    "            return concat_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(encoder, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = training.StandardUpdater(\n",
    "    train_iter, optimizer,  converter=convert_seq, device=-1)\n",
    "trainer = training.Trainer(updater, (10, 'epoch'), out='result')\n",
    "\n",
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer.extend(extensions.Evaluator(\n",
    "    test_iter, model,  converter=convert_seq, device=-1))\n",
    "\n",
    "# Take a best snapshot\n",
    "record_trigger = training.triggers.MaxValueTrigger(\n",
    "    'validation/main/accuracy', (1, 'epoch'))\n",
    "trainer.extend(extensions.snapshot_object(\n",
    "    model, 'best_model.npz'),\n",
    "    trigger=record_trigger)\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "# Print a progress bar to stdout\n",
    "trainer.extend(extensions.ProgressBar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J     total [####..............................................]  9.09%\n",
      "this epoch [#############################################.....] 90.91%\n",
      "       100 iter, 0 epoch / 10 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
      "\u001b[4A\u001b[J1           2.79333     2.66317               0.318545       0.329309                  86.1012       \n",
      "\u001b[J     total [#########.........................................] 18.18%\n",
      "this epoch [########################################..........] 81.82%\n",
      "       200 iter, 1 epoch / 10 epochs\n",
      "    1.3784 iters/sec. Estimated time to finish: 0:10:52.933378.\n",
      "\u001b[4A\u001b[J2           2.51246     2.51589               0.341364       0.34023                   164.085       \n",
      "\u001b[J     total [#############.....................................] 27.27%\n",
      "this epoch [####################################..............] 72.73%\n",
      "       300 iter, 2 epoch / 10 epochs\n",
      "    1.3931 iters/sec. Estimated time to finish: 0:09:34.256424.\n",
      "\u001b[4A\u001b[J3           2.34539     2.42072               0.362273       0.354309                  238.792       \n",
      "\u001b[J     total [##################................................] 36.36%\n",
      "this epoch [###############################...................] 63.64%\n",
      "       400 iter, 3 epoch / 10 epochs\n",
      "    1.4537 iters/sec. Estimated time to finish: 0:08:01.541926.\n",
      "\u001b[4A\u001b[J4           2.1736      2.31368               0.388455       0.362877                  308.538       \n",
      "\u001b[J     total [######################............................] 45.45%\n",
      "this epoch [###########################.......................] 54.55%\n",
      "       500 iter, 4 epoch / 10 epochs\n",
      "    1.4486 iters/sec. Estimated time to finish: 0:06:54.191725.\n",
      "\u001b[4A\u001b[J5           1.98792     2.30383               0.428545       0.36121                   387.466       \n",
      "\u001b[J     total [###########################.......................] 54.55%\n",
      "this epoch [######################............................] 45.45%\n",
      "       600 iter, 5 epoch / 10 epochs\n",
      "    1.4336 iters/sec. Estimated time to finish: 0:05:48.777451.\n",
      "\u001b[4A\u001b[J6           1.8107      2.24025               0.473182       0.356975                  470.722       \n",
      "\u001b[J     total [###############################...................] 63.64%\n",
      "this epoch [##################................................] 36.36%\n",
      "       700 iter, 6 epoch / 10 epochs\n",
      "     1.417 iters/sec. Estimated time to finish: 0:04:42.282552.\n",
      "\u001b[4A\u001b[J7           1.65306     2.28988               0.519091       0.366564                  545.079       \n",
      "\u001b[J     total [####################################..............] 72.73%\n",
      "this epoch [#############.....................................] 27.27%\n",
      "       800 iter, 7 epoch / 10 epochs\n",
      "     1.422 iters/sec. Estimated time to finish: 0:03:30.969567.\n",
      "\u001b[4A\u001b[J8           1.47925     2.24131               0.570818       0.375564                  623.782       \n",
      "\u001b[J     total [########################################..........] 81.82%\n",
      "this epoch [#########.........................................] 18.18%\n",
      "       900 iter, 8 epoch / 10 epochs\n",
      "    1.4157 iters/sec. Estimated time to finish: 0:02:21.269566.\n",
      "\u001b[4A\u001b[J9           1.35065     2.30046               0.609          0.360074                  703.43        \n",
      "\u001b[J     total [#############################################.....] 90.91%\n",
      "this epoch [####..............................................]  9.09%\n",
      "      1000 iter, 9 epoch / 10 epochs\n",
      "    1.4119 iters/sec. Estimated time to finish: 0:01:10.827540.\n",
      "\u001b[4A\u001b[J10          1.19984     2.35786               0.652455       0.367642                  777.812       \n",
      "\u001b[J     total [##################################################] 100.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "      1100 iter, 10 epoch / 10 epochs\n",
      "    1.4188 iters/sec. Estimated time to finish: 0:00:00.\n",
      "\u001b[4A\u001b[J"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import Chain\n",
    "from chainer import reporter\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "class BiLSTMBase(Chain):\n",
    "    def __init__(self, input_size, n_labels, dropout=0.5):\n",
    "        super(BiLSTMBase, self).__init__()\n",
    "        with self.init_scope():\n",
    "      # self.f_lstm = L.LSTM(None, feature_size, dropout)\n",
    "      # self.b_lstm = L.LSTM(None, feature_size, dropout)\n",
    "            self.nstep_bilstm = L.NStepBiLSTM(n_layers=1, in_size=input_size, out_size=input_size, dropout=dropout)\n",
    "            self.l1 = L.Linear(input_size*2, n_labels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def __call__(self, xs, ys):\n",
    "        pred_ys = self.traverse(xs)\n",
    "        \n",
    "        loss = .0\n",
    "        for pred_y, y in zip(pred_ys, ys):\n",
    "            _loss = F.softmax_cross_entropy(pred_y, y)\n",
    "            loss += _loss\n",
    "        reporter.report({'loss': loss.data}, self)\n",
    "        \n",
    "        accuracy = .0\n",
    "        pred_ys = [F.softmax(pred_y) for pred_y in pred_ys]\n",
    "        pred_ys = [pred_y.data.argmax(axis=0)[1] for pred_y in pred_ys]\n",
    "        ys = [y.argmax(axis=0) for y in ys]\n",
    "        for pred_y, y in zip(pred_ys, ys):\n",
    "            if y == pred_y:\n",
    "                accuracy += 1/len(ys)\n",
    "        reporter.report({'accuracy': accuracy}, self)\n",
    "        return loss\n",
    "\n",
    "    def traverse(self, xs):\n",
    "        hx, cx = None, None\n",
    "        hx, cx, ys = self.nstep_bilstm(xs=xs, hx=hx, cx=cx)\n",
    "        return [self.l1(y) for y in ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMBase(input_size=234, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import convert_seq\n",
    "train_iter = chainer.iterators.SerialIterator(dataset[:11000], batchsize)\n",
    "test_iter = chainer.iterators.SerialIterator(dataset[11000:], batchsize,\n",
    "                                             repeat=False, shuffle=False)\n",
    "\n",
    "# Setup an optimizer\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "\n",
    "# Set up a trainer\n",
    "updater = training.StandardUpdater(\n",
    "    train_iter, optimizer, converter=convert_seq, device=-1)\n",
    "trainer = training.Trainer(updater, (10, 'epoch'), out='result')\n",
    "\n",
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer.extend(extensions.Evaluator(\n",
    "    test_iter, model, converter=convert_seq, device=-1))\n",
    "\n",
    "# Take a best snapshot\n",
    "record_trigger = training.triggers.MaxValueTrigger(\n",
    "    'validation/main/accuracy', (1, 'epoch'))\n",
    "trainer.extend(extensions.snapshot_object(\n",
    "    model, 'best_model.npz'),\n",
    "    trigger=record_trigger)\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "# Print a progress bar to stdout\n",
    "trainer.extend(extensions.ProgressBar())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J     total [####..............................................]  9.09%\n",
      "this epoch [#############################################.....] 90.91%\n",
      "       100 iter, 0 epoch / 10 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
      "\u001b[4A\u001b[J1           12.2332     9.90821               0.370364       0.470169                  158.395       \n",
      "\u001b[J     total [#########.........................................] 18.18%\n",
      "this epoch [########################################..........] 81.82%\n",
      "       200 iter, 1 epoch / 10 epochs\n",
      "   0.76206 iters/sec. Estimated time to finish: 0:19:41.006930.\n",
      "\u001b[4A\u001b[J2           9.11983     9.26177               0.502818       0.50607                   301.96        \n",
      "\u001b[J     total [#############.....................................] 27.27%\n",
      "this epoch [####################################..............] 72.73%\n",
      "       300 iter, 2 epoch / 10 epochs\n",
      "   0.75302 iters/sec. Estimated time to finish: 0:17:42.382031.\n",
      "\u001b[4A\u001b[J3           7.82877     8.4579                0.574091       0.517679                  450.922       \n",
      "\u001b[J     total [##################................................] 36.36%\n",
      "this epoch [###############################...................] 63.64%\n",
      "       400 iter, 3 epoch / 10 epochs\n",
      "   0.74403 iters/sec. Estimated time to finish: 0:15:40.816063.\n",
      "\u001b[4A\u001b[J4           6.70992     8.17156               0.634091       0.556305                  599.779       \n",
      "\u001b[J     total [######################............................] 45.45%\n",
      "this epoch [###########################.......................] 54.55%\n",
      "       500 iter, 4 epoch / 10 epochs\n",
      "   0.74151 iters/sec. Estimated time to finish: 0:13:29.161739.\n",
      "\u001b[4A\u001b[J5           5.71199     8.42587               0.694273       0.574971                  750.983       \n",
      "\u001b[J     total [###########################.......................] 54.55%\n",
      "this epoch [######################............................] 45.45%\n",
      "       600 iter, 5 epoch / 10 epochs\n",
      "   0.73429 iters/sec. Estimated time to finish: 0:11:20.928103.\n",
      "\u001b[4A\u001b[J6           4.70364     8.51549               0.754091       0.560893                  904.755       \n",
      "\u001b[J     total [###############################...................] 63.64%\n",
      "this epoch [##################................................] 36.36%\n",
      "       700 iter, 6 epoch / 10 epochs\n",
      "   0.73067 iters/sec. Estimated time to finish: 0:09:07.439062.\n",
      "\u001b[4A\u001b[J7           3.99978     9.27681               0.785          0.518148                  1077.2        \n",
      "\u001b[J     total [####################################..............] 72.73%\n",
      "this epoch [#############.....................................] 27.27%\n",
      "       800 iter, 7 epoch / 10 epochs\n",
      "   0.71506 iters/sec. Estimated time to finish: 0:06:59.547411.\n",
      "\u001b[4A\u001b[J8           3.33322     9.47921               0.823818       0.580638                  1228.03       \n",
      "\u001b[J     total [########################################..........] 81.82%\n",
      "this epoch [#########.........................................] 18.18%\n",
      "       900 iter, 8 epoch / 10 epochs\n",
      "   0.71399 iters/sec. Estimated time to finish: 0:04:40.117515.\n",
      "\u001b[4A\u001b[J9           2.77367     9.86491               0.856364       0.56956                   1388.65       \n",
      "\u001b[J     total [#############################################.....] 90.91%\n",
      "this epoch [####..............................................]  9.09%\n",
      "      1000 iter, 9 epoch / 10 epochs\n",
      "   0.71247 iters/sec. Estimated time to finish: 0:02:20.356985.\n",
      "\u001b[4A\u001b[J10          2.37635     10.4935               0.879182       0.563716                  1542.41       \n",
      "\u001b[J     total [##################################################] 100.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "      1100 iter, 10 epoch / 10 epochs\n",
      "   0.71183 iters/sec. Estimated time to finish: 0:00:00.\n",
      "\u001b[4A\u001b[J"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ys = model.traverse([xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-89d4f556845a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnstep_bilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/chainer/links/connection/n_step_lstm.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, hx, cx, xs, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_kwargs_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_step_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort_list_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.nstep_bilstm(xs=xs, hx=None, cx=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ys = [F.softmax(pred_y) for pred_y in pred_ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[variable([[-9.23650414e-02,  1.37938619e-01],\n",
       "           [-1.59259275e-01,  1.12848647e-01],\n",
       "           [-1.26221865e-01,  1.58787176e-01],\n",
       "           [ 3.71674359e-01,  6.14576861e-02],\n",
       "           [ 1.99825794e-01, -3.85012478e-01],\n",
       "           [ 2.04078197e-01, -2.53929496e-01],\n",
       "           [ 3.04586053e-01,  3.18914711e-01],\n",
       "           [ 3.23670924e-01,  3.59954953e-01],\n",
       "           [ 2.28468806e-01,  5.96089736e-02],\n",
       "           [ 7.04916120e-02,  1.95891261e-01],\n",
       "           [ 1.34538442e-01,  2.49079287e-01],\n",
       "           [ 3.89353603e-01,  2.32209042e-01],\n",
       "           [-6.84104264e-02, -2.30646729e-02],\n",
       "           [ 4.19745982e-01, -4.42816466e-02],\n",
       "           [ 3.21061641e-01, -8.94789845e-02],\n",
       "           [ 3.37609887e-01,  2.99052000e-01],\n",
       "           [ 9.12030786e-02,  5.33087194e-01],\n",
       "           [ 3.08924198e-01,  2.80614078e-01],\n",
       "           [ 1.37659743e-01, -1.45233870e-02],\n",
       "           [ 2.95186639e-01,  1.68519124e-01],\n",
       "           [ 3.30723003e-02, -1.08975716e-01],\n",
       "           [-2.45240957e-01,  3.11560929e-04],\n",
       "           [ 7.71331489e-02,  2.19074070e-01],\n",
       "           [ 2.11459920e-02,  7.80418962e-02],\n",
       "           [ 4.18423504e-01, -5.71237579e-02],\n",
       "           [-1.19796693e-02,  1.24653839e-01],\n",
       "           [ 8.07914361e-02, -9.20103490e-02],\n",
       "           [-1.35642961e-01,  4.70422283e-02],\n",
       "           [ 2.30504096e-01,  2.45052665e-01],\n",
       "           [ 4.26935703e-01,  4.65564877e-02],\n",
       "           [-1.08742014e-01, -2.15043932e-01],\n",
       "           [ 2.55534708e-01, -1.33269280e-01]])]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ys = [F.softmax(pred_y) for pred_y in pred_ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ys[0].data.argmax(axis=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
